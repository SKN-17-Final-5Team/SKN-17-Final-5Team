"""
ë¬´ì—­ ë¬¸ì„œ ê²€ìƒ‰ Tool

ë³µí•© ì§ˆë¬¸ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì¿¼ë¦¬ ë³€í™˜ + ë³‘ë ¬ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€
- ì¿¼ë¦¬ ê°œì„ : "ë¬´ì—­ ì‚¬ê¸° ë°©ì§€ ì–´ë–»ê²Œ í•´?" â†’ "ë¬´ì—­ ì‚¬ê¸° ì˜ˆë°© ë° ëŒ€ì‘ ë°©ë²•"
- ë³µí•© ì§ˆë¬¸ ë¶„í•´: "ìˆ˜ì¶œê³¼ ìˆ˜ì… ì°¨ì´" â†’ ["ìˆ˜ì¶œ ì ˆì°¨", "ìˆ˜ì… ì ˆì°¨"] 2ê°œë¡œ ë‚˜ëˆ ì„œ ê²€ìƒ‰
- ë³‘ë ¬ ê²€ìƒ‰: ì—¬ëŸ¬ ì„œë¸Œì¿¼ë¦¬ë¥¼ ë™ì‹œì— ê²€ìƒ‰í•´ì„œ ì†ë„ í–¥ìƒ
- Reranking: ìµœì¢…ì ìœ¼ë¡œ ê´€ë ¨ë„ ë†’ì€ ë¬¸ì„œë§Œ Agentì—ê²Œ ì „ë‹¬
"""

import asyncio
from typing import List
from agents import function_tool

from config import (
    qdrant_client,
    openai_client,
    COLLECTION_NAME,
    EMBEDDING_MODEL,
    USE_RERANKER
)
from utils import print_retrieved_documents
from services.reranker_service import call_reranker_api
from services.query_transformer_service import rewrite_and_decompose_query


@function_tool
async def search_trade_documents(query: str, limit: int = 25, top_k: int = 5) -> str:
    """
    ë¬´ì—­ ë¬¸ì„œ ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜

    ë‹¨ìˆœ ì§ˆë¬¸("ìˆ˜ì¶œ ì ˆì°¨ëŠ”?")ë„, ë³µí•© ì§ˆë¬¸("ìˆ˜ì¶œê³¼ ìˆ˜ì… ì°¨ì´ëŠ”?")ë„ ëª¨ë‘ ì²˜ë¦¬ ê°€ëŠ¥

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        limit: Qdrantì—ì„œ ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 25ê°œ)
        top_k: ìµœì¢…ì ìœ¼ë¡œ Agentì—ê²Œ ì „ë‹¬í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 5ê°œ)

    Returns:
        Agentê°€ ì½ì„ ìˆ˜ ìˆê²Œ í¬ë§·ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸
    """
    print(f"\nğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}' (ì´ˆê¸° ê²€ìƒ‰: {limit}ê°œ, ìµœì¢… ì„ ì •: {top_k}ê°œ)")

    # ì¿¼ë¦¬ ê°œì„  + í•„ìš”í•˜ë©´ ë³µí•© ì§ˆë¬¸ ë¶„í•´
    # ì˜ˆ: "ìˆ˜ì¶œ ìˆ˜ì… ì°¨ì´" â†’ rewritten_query + sub_queries 2ê°œ
    transform = await rewrite_and_decompose_query(query)
    rewritten_query = transform.rewritten_query
    sub_queries = transform.sub_queries

    # ë‹¨ìˆœ ì§ˆë¬¸ì´ë©´ ê·¸ëƒ¥ ê²€ìƒ‰, ë³µí•© ì§ˆë¬¸ì´ë©´ ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ê°œ ê²€ìƒ‰
    if not sub_queries or len(sub_queries) == 0:
        points = await _single_search(rewritten_query, limit)
    else:
        # ì—¬ëŸ¬ ì„œë¸Œì¿¼ë¦¬ë¥¼ ë™ì‹œì— ê²€ìƒ‰ â†’ ì¤‘ë³µ ì œê±° â†’ ë³‘í•©
        points = await _multi_search(sub_queries, limit)

    print(f"âœ“ ìµœì¢… {len(points)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘\n")

    if not points:
        print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

    # ë””ë²„ê¹…ìš©: ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥ (ì½˜ì†”ì—ë§Œ í‘œì‹œ, Agentì—ê²ŒëŠ” ì•ˆ ë³´ëƒ„)
    print_retrieved_documents(points, n=25)

    # Rerankerì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents_for_rerank = [
        point.payload.get("text") or point.payload.get("content") or ""
        for point in points
    ]

    # Rerankerë¡œ ì¬ì •ë ¬ (ì„¤ì •ì—ì„œ ì¼œë†¨ìœ¼ë©´)
    rerank_response = None

    if USE_RERANKER:
        try:
            # rewritten_queryë¡œ rerank (ì›ë³¸ queryë³´ë‹¤ ë” ì •í™•í•¨)
            rerank_response = await call_reranker_api(rewritten_query, documents_for_rerank, top_k=top_k)
        except Exception as e:
            print(f"âš ï¸  Reranker ì‹¤íŒ¨: {e}")
            print(f"âš ï¸  ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ì˜ ìƒìœ„ {top_k}ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n")
            rerank_response = None
    else:
        print(f"â„¹ï¸  Reranker ë¯¸ì‚¬ìš© - ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ {top_k}ê°œ ì‚¬ìš©\n")

    # Agentì—ê²Œ ì „ë‹¬í•  ìµœì¢… ë¬¸ì„œ í¬ë§·íŒ…
    if rerank_response:
        print("="*60)
        print(f"ğŸ¯ Rerankerë¡œ ì„ ì •ëœ ìµœì¢… {len(rerank_response.results)}ê°œ ë¬¸ì„œ (ëª¨ë¸ì—ê²Œ ì „ë‹¬)")
        print("="*60)

        formatted = []
        for rank, result in enumerate(rerank_response.results, 1):
            original_point = points[result.index]
            content = original_point.payload.get("text") or original_point.payload.get("content") or ""
            if content:
                content = content[:500]  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ„
            source_tag = original_point.payload.get("data_source", "unknown")
            rerank_score = result.score

            # Agentì—ê²Œ ì „ë‹¬í•  í…ìŠ¤íŠ¸ (ê°„ê²°í•˜ê²Œ)
            doc_text = f"[{rank}] {content}\n   ì¶œì²˜: {source_tag}, Rerank ì ìˆ˜: {rerank_score:.3f}"
            formatted.append(doc_text)

            # ì½˜ì†”ì—ë§Œ ì¶”ê°€ ì •ë³´ ì¶œë ¥ (ê°œë°œì ë””ë²„ê¹…ìš©)
            debug_doc_name = original_point.payload.get("document_name") or original_point.payload.get("file_name")
            debug_article = original_point.payload.get("article")

            print(f"\në¬¸ì„œ {rank}:")
            print(f"  ì¶œì²˜: {source_tag}")
            if debug_doc_name:
                print(f"  íŒŒì¼ëª…: {debug_doc_name}")
            if debug_article:
                print(f"  ì¡°ë¬¸: {debug_article}")
            print(f"  ì›ë³¸ ì¸ë±ìŠ¤: {result.index + 1}")
            print(f"  Rerank ì ìˆ˜: {rerank_score:.3f}")
            print(f"  ë‚´ìš©: {content[:200]}{'...' if len(content) > 200 else ''}")

    else:
        # Reranker ì‹¤íŒ¨í–ˆê±°ë‚˜ êº¼ì ¸ìˆìœ¼ë©´ ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©
        print("="*60)
        print(f"ğŸ“„ ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ {top_k}ê°œ (ëª¨ë¸ì—ê²Œ ì „ë‹¬)")
        print("="*60)

        formatted = []
        for i, point in enumerate(points[:top_k], 1):
            content = point.payload.get("text") or point.payload.get("content") or ""
            if content:
                content = content[:500]
            score = point.score
            source_tag = point.payload.get("data_source", "unknown")

            doc_text = f"[{i}] {content}\n   ì¶œì²˜: {source_tag}, ì ìˆ˜: {score:.3f}"
            formatted.append(doc_text)

    print("\n" + "=" * 60)
    print("ğŸ¤– ëª¨ë¸ì´ ìœ„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
    print("=" * 60 + "\n")

    return "\n\n".join(formatted)


# ===== ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ =====

async def _single_search(query: str, limit: int) -> List:
    """
    ì¼ë°˜ì ì¸ ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ (ë³µí•© ì§ˆë¬¸ ì•„ë‹ ë•Œ)
    ì¿¼ë¦¬ â†’ Embedding â†’ Qdrant ê²€ìƒ‰ â†’ ê²°ê³¼ ë°˜í™˜
    """
    print(f"ğŸ“Œ ë‹¨ì¼ ê²€ìƒ‰ ìˆ˜í–‰: '{query}'")

    # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
    response = openai_client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=query
    )
    query_vector = response.data[0].embedding

    # Qdrantì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    search_result = qdrant_client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_vector,
        limit=limit,
        with_payload=True
    )

    points = search_result.points if hasattr(search_result, 'points') else []
    print(f"   â†’ {len(points)}ê°œ ë¬¸ì„œ ë°œê²¬")

    return points


async def _multi_search(sub_queries: List[str], limit: int) -> List:
    """
    ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬ìš© ë³‘ë ¬ ê²€ìƒ‰

    ì˜ˆ: ["ìˆ˜ì¶œ ì ˆì°¨", "ìˆ˜ì… ì ˆì°¨"] 2ê°œë¥¼ ë™ì‹œì— ê²€ìƒ‰ â†’ ì¤‘ë³µ ì œê±° â†’ ë³‘í•©
    ìˆœì°¨ ê²€ìƒ‰ë³´ë‹¤ 2~3ë°° ë¹ ë¦„ (asyncio.gather ë•ë¶„)
    """
    print(f"ğŸ“Œ ë©€í‹° ê²€ìƒ‰ ìˆ˜í–‰ ({len(sub_queries)}ê°œ ì„œë¸Œì¿¼ë¦¬)")

    # 1) ëª¨ë“  ì„œë¸Œì¿¼ë¦¬ë¥¼ ë™ì‹œì— ë²¡í„°ë¡œ ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬)
    print("   Step 1: Embedding ìƒì„± ì¤‘...")
    embedding_tasks = [
        asyncio.to_thread(  # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ê°ì‹¸ê¸°
            openai_client.embeddings.create,
            model=EMBEDDING_MODEL,
            input=sq
        )
        for sq in sub_queries
    ]
    embeddings = await asyncio.gather(*embedding_tasks)  # ëª¨ë‘ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°

    # 2) ëª¨ë“  ë²¡í„°ë¡œ ë™ì‹œì— Qdrant ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬)
    print("   Step 2: Qdrant ê²€ìƒ‰ ì¤‘...")
    search_tasks = [
        asyncio.to_thread(
            qdrant_client.query_points,
            collection_name=COLLECTION_NAME,
            query=emb.data[0].embedding,
            limit=limit,
            with_payload=True
        )
        for emb in embeddings
    ]
    search_results = await asyncio.gather(*search_tasks)

    # 3) ê° ì„œë¸Œì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
    for i, (sq, result) in enumerate(zip(sub_queries, search_results), 1):
        points_count = len(result.points) if hasattr(result, 'points') else 0
        print(f"   ì„œë¸Œì¿¼ë¦¬ {i}: '{sq}' â†’ {points_count}ê°œ")

    # 4) ì¤‘ë³µ ë¬¸ì„œ ì œê±° (ê°™ì€ ë¬¸ì„œê°€ ì—¬ëŸ¬ ì„œë¸Œì¿¼ë¦¬ì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
    print("   Step 4: ì¤‘ë³µ ì œê±° ë° ë³‘í•© ì¤‘...")
    seen_ids = {}

    for result in search_results:
        points = result.points if hasattr(result, 'points') else []
        for point in points:
            point_id = point.id
            # ê°™ì€ ë¬¸ì„œë©´ ì ìˆ˜ê°€ ë” ë†’ì€ ìª½ìœ¼ë¡œ ë³´ì¡´
            if point_id not in seen_ids or point.score > seen_ids[point_id].score:
                seen_ids[point_id] = point

    # 5) ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    merged_points = sorted(seen_ids.values(), key=lambda p: p.score, reverse=True)

    total_before = sum(
        len(result.points) if hasattr(result, 'points') else 0
        for result in search_results
    )
    print(f"   â†’ ì¤‘ë³µ ì œê±° ì „: {total_before}ê°œ, í›„: {len(merged_points)}ê°œ")

    # Rerankerê°€ ë‹¤ì‹œ ê³¨ë¼ë‚¼ê±°ë‹ˆê¹Œ ë„‰ë„‰íˆ ì „ë‹¬ (limitì˜ 2ë°°)
    return merged_points[:limit * 2]
