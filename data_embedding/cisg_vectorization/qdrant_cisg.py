import json
import re
import os
import hashlib
from pathlib import Path
from itertools import groupby
from dotenv import load_dotenv

# RAG ê´€ë ¨
import openai
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# Deprecation ê²½ê³  ë¬´ì‹œ
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning) 

# ----------------------------------------------------
# ì—…ë¡œë“œ ì„¤ì •: ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
# ----------------------------------------------------
class CONFIG_UPLOAD:
    # --- 1. íŒŒì¼ ê²½ë¡œ ì„¤ì • ---
    BASE_PATH = Path(__file__).parent.parent.parent  # ex) /Users/hoon/Desktop/final_project
    DOCUMENT_PATH = BASE_PATH / "data/extracted_data/ë‹¤ìì¡°ì•½ìƒì„¸(CISG).txt"
    BASE_CHUNKS_PATH = BASE_PATH / "data_embedding/cisg_vectorization/used_data/cisg_chunks.json"

    # --- 2. Qdrant ì»¬ë ‰ì…˜ ì´ë¦„ ì„¤ì • ---
    # ë°ì´í„°ë¥¼ ì €ì¥í•  Qdrant Cloudì˜ ì»¬ë ‰ì…˜(í…Œì´ë¸”) ì´ë¦„ì…ë‹ˆë‹¤.
    # ì˜ˆ: "cisg_production_v1"
    COLLECTION_NAME = "trade_collection"

    # --- 3. ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì„ íƒ ---
    # OpenAI ì„ë² ë”© ëª¨ë¸ (í•˜ë‚˜ë§Œ ì„ íƒ)
    MODEL_NAME = "openai_text-embedding-3-large"
    # MODEL_NAME = "openai_text-embedding-ada-002"

    # --- 4. ì‚¬ìš©í•  ì²­í‚¹ ì „ëµ ì„ íƒ (í•˜ë‚˜ë§Œ ì„ íƒ) ---
    # "Ho_Segmented": ê°€ì¥ ì„¸ë¶„í™”ëœ ë‹¨ìœ„ (ê¸°ë³¸)
    # "Paragraph": 'í•­' ë‹¨ìœ„ë¡œ ë³‘í•©
    # "Article": 'ì¡°' ë‹¨ìœ„ë¡œ ë³‘í•©
    CHUNK_STRATEGY = "Article"

    # --- 5. í™˜ê²½ ë³€ìˆ˜ í‚¤ ì´ë¦„ (.env íŒŒì¼ì—ì„œ ë¡œë“œ) ---
    QDRANT_URL_KEY = "QDRANT_URL"
    QDRANT_API_KEY = "QDRANT_API_KEY"
    OPENAI_API_KEY = "OPENAI_API_KEY"



# ----------------------------------------------------
# ëª¨ë¸ í•¸ë“¤ëŸ¬ ì •ì˜ (OpenAI)
# ----------------------------------------------------
def get_model_handler(model_name: str, keys: dict) -> dict:
    """OpenAI ì„ë² ë”© ëª¨ë¸ì— ë”°ë¼ ì„ë² ë”© í•¨ìˆ˜ì™€ ì°¨ì›ì„ ì¤€ë¹„í•©ë‹ˆë‹¤."""

    print(f"  [ëª¨ë¸ ë¡œë”] '{model_name}' ë¡œë“œ ì¤‘...")

    if not model_name.startswith("openai_"):
        raise ValueError(f"OpenAI ëª¨ë¸ë§Œ ì§€ì›í•©ë‹ˆë‹¤. 'openai_'ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë¸ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”: {model_name}")

    model_id = model_name.split('_', 1)[1]
    client = openai.OpenAI(api_key=keys['openai'])

    # ëª¨ë¸ë³„ ì°¨ì› ì„¤ì •
    if model_id == "text-embedding-3-large":
        dim = 3072
    elif model_id == "text-embedding-ada-002":
        dim = 1536
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” OpenAI ëª¨ë¸ì…ë‹ˆë‹¤: {model_id}")

    # ì„ë² ë”© í•¨ìˆ˜ ì •ì˜
    def embed_texts(texts: list) -> list:
        response = client.embeddings.create(input=texts, model=model_id)
        return [item.embedding for item in response.data]

    print(f"  [ëª¨ë¸ ë¡œë”] OpenAI '{model_id}' í•¸ë“¤ëŸ¬ ìƒì„± ì™„ë£Œ (ì°¨ì›: {dim})")
    return {
        "name": model_name,
        "embed_texts": embed_texts,
        "dim": dim
    }



# ----------------------------------------------------
# ë°ì´í„° ë¡œë“œ ë° ì •ì œ í•¨ìˆ˜
# ----------------------------------------------------
def load_document(path) -> str:
    """ì›ë³¸ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. Path ê°ì²´ ë˜ëŠ” ë¬¸ìì—´ì„ ë°›ìŠµë‹ˆë‹¤."""
    path = Path(path)  # Path ê°ì²´ë¡œ ë³€í™˜
    print(f"[LOAD] ì›ë³¸ ë¬¸ì„œ ë¡œë“œ: {path}")
    with open(path, encoding="utf-8") as f:
        text = f.read()
    return text

def load_base_chunks(path) -> list:
    """ê¸°ë°˜ ì²­í¬ JSONì„ ë¡œë“œí•©ë‹ˆë‹¤. Path ê°ì²´ ë˜ëŠ” ë¬¸ìì—´ì„ ë°›ìŠµë‹ˆë‹¤."""
    path = Path(path)  # Path ê°ì²´ë¡œ ë³€í™˜
    print(f"[LOAD] ê¸°ë°˜(Base) ì²­í¬ JSON ë¡œë“œ: {path}")
    with open(path, encoding="utf-8") as f:
        chunks = json.load(f)
    return chunks

def attach_chunk_spans(text: str, chunks: list) -> list:
    """ì²­í¬ì˜ contentë¥¼ ì›ë³¸ textì™€ ë¹„êµí•˜ì—¬ 'start', 'end' ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    print(f"  [SPAN] ì²­í¬ ìœ„ì¹˜(span) ê³„ì‚° ì¤‘...")
    current_search_idx, not_found = 0, 0
    new_chunks = []
    skipped_chunks = []  # ëˆ„ë½ëœ ì²­í¬ ì¶”ì 

    for i, chunk in enumerate(chunks):
        chunk_id = chunk.get("chunk_id")
        if not chunk_id:
            not_found += 1
            skipped_chunks.append({"index": i, "reason": "chunk_id ì—†ìŒ"})
            continue
        chunk["id"] = chunk_id

        json_content = chunk.get("content")
        if not json_content:
            not_found += 1
            skipped_chunks.append({"index": i, "chunk_id": chunk_id, "reason": "content ì—†ìŒ"})
            continue

        start = text.find(json_content, current_search_idx)
        if start == -1:
            temp_content = re.sub(r'\s+', ' ', json_content).strip()
            start = text.find(temp_content, current_search_idx)
        if start == -1:
            print(f"    âš  ì²­í¬ #{i} ('{chunk_id}') span ì°¾ê¸° ì‹¤íŒ¨. ìŠ¤í‚µ.")
            not_found += 1
            skipped_chunks.append({
                "index": i,
                "chunk_id": chunk_id,
                "reason": "ì›ë³¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                "content_preview": json_content[:100] + "..." if len(json_content) > 100 else json_content
            })
            continue

        end = start + len(json_content)
        chunk["text"] = text[start:end]  # 'text' í•„ë“œ í‘œì¤€í™”
        chunk["start"] = start
        chunk["end"] = end
        new_chunks.append(chunk)
        current_search_idx = end

    # ëˆ„ë½ëœ ì²­í¬ê°€ ìˆìœ¼ë©´ ê²½ê³  ì¶œë ¥
    if skipped_chunks:
        print(f"    âš ï¸  ëˆ„ë½ëœ ì²­í¬ ìƒì„¸:")
        for skip in skipped_chunks[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            print(f"       - ì¸ë±ìŠ¤ {skip['index']}: {skip['reason']}")
        if len(skipped_chunks) > 5:
            print(f"       ... ì™¸ {len(skipped_chunks) - 5}ê°œ ë”")

    print(f"    [SPAN] ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ë°˜ ì²­í¬: {len(new_chunks)}ê°œ (ëˆ„ë½: {not_found}ê°œ)")
    return new_chunks



# ----------------------------------------------------
# ì²­í¬ ë³‘í•© í•¨ìˆ˜
# ----------------------------------------------------
def merge_chunks(base_chunks: list, strategy_name: str, raw_text: str) -> list:
    """'Ho_Segmented' ë‹¨ìœ„ì˜ ì²­í¬ë¥¼ ë” í° ë‹¨ìœ„('Paragraph', 'Article')ë¡œ ë³‘í•©í•©ë‹ˆë‹¤."""
    print(f"  [Chunking] ì „ëµ '{strategy_name}' ì‹¤í–‰ ì¤‘...")
    
    if strategy_name == "Ho_Segmented":
        for chunk in base_chunks:
            chunk['strategy_name'] = strategy_name
            # ë°ì´í„°ì†ŒìŠ¤  êµ¬ë¶„ìš© ë©”íƒ€ë°ì´í„°
            chunk['data_source'] = 'cisg'
            # ë™ì¼í•˜ê²Œ ìƒì„±ë˜ëŠ” ID ìƒì„±
            if 'id' not in chunk or len(chunk.get('id', '')) > 20:
                id_string = f"CISG_{chunk.get('article', 'unknown')}_{chunk.get('paragraph_no', 'unknown')}_{chunk.get('ho_no', 'unknown')}"
                chunk['id'] = hashlib.md5(id_string.encode()).hexdigest()
        print(f"    [Chunking] '{strategy_name}' ì™„ë£Œ. {len(base_chunks)}ê°œ ì²­í¬ ì‚¬ìš©.")
        return base_chunks

    if strategy_name == "Paragraph":
        get_key = lambda x: (x.get('article'), x.get('paragraph_no'))
    elif strategy_name == "Article":
        get_key = lambda x: x.get('article')
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë³‘í•© ì „ëµ: {strategy_name}")

    merged_chunks = []
    base_chunks.sort(key=lambda x: x['start'])
    
    for key, group in groupby(base_chunks, key=get_key):
        if key is None or (isinstance(key, tuple) and None in key):
            continue
            
        group_list = list(group)
        first_chunk = group_list[0]
        last_chunk = group_list[-1]
        min_start, max_end = first_chunk['start'], last_chunk['end']
        merged_text = raw_text[min_start:max_end]

        if strategy_name == "Paragraph":
            # Paragraph: CISG_Article_{article}_Paragraph_{paragraph_no}
            id_string = f"CISG_Article_{first_chunk.get('article')}_Paragraph_{first_chunk.get('paragraph_no')}"
        else:  # Article
            # Article: CISG_Article_{article}
            id_string = f"CISG_Article_{first_chunk.get('article')}"

        deterministic_id = hashlib.md5(id_string.encode()).hexdigest()

        new_chunk = {
            "part": first_chunk.get("part"),
            "chapter": first_chunk.get("chapter"),
            "article": first_chunk.get("article"),
            "paragraph_no": first_chunk.get("paragraph_no") if strategy_name == "Paragraph" else None,
            "text": merged_text,
            "start": min_start,
            "end": max_end,
            "id": deterministic_id,  # Deterministic ID based on content
            "strategy_name": strategy_name,
            "data_source": "cisg",  # Add source metadata
        }
        merged_chunks.append(new_chunk)
        
    print(f"    [Chunking] '{strategy_name}' ì™„ë£Œ. {len(merged_chunks)}ê°œ ì²­í¬ ìƒì„±.")
    return merged_chunks



# ----------------------------------------------------
# Qdrant ì—…ë¡œë“œ í•¨ìˆ˜ (í•µì‹¬ ë¡œì§)
# ----------------------------------------------------
def create_collection_if_not_exists(client: QdrantClient, collection_name: str, vector_size: int):
    """
    [ì¤‘ìš”] 'recreate_collection'(ì‚­ì œ í›„ ìƒì„±) ëŒ€ì‹ , 
    ì»¬ë ‰ì…˜ì´ ì—†ì„ ë•Œë§Œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. (ë°ì´í„° ì¶”ê°€/append ë³´ì¥)
    """
    if client.collection_exists(collection_name):
        print(f"  [QDRANT] ì»¬ë ‰ì…˜ '{collection_name}'ì´(ê°€) ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì¶”ê°€(upsert)í•©ë‹ˆë‹¤.")
    else:
        print(f"  [QDRANT] ìƒˆ ì»¬ë ‰ì…˜ '{collection_name}'ì„(ë¥¼) ìƒì„±í•©ë‹ˆë‹¤. (dim={vector_size})")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
        )

def upload_to_qdrant(client: QdrantClient, collection_name: str, model_handler: dict, chunks: list, batch_size: int = 20):
    """
    ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ì—¬ Qdrantì— 'upsert' (ì¶”ê°€ ë˜ëŠ” ë®ì–´ì“°ê¸°)í•©ë‹ˆë‹¤.
    ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    """
    texts = [c["text"] for c in chunks]
    print(f"    [QDRANT] ì„ë² ë”© ê³„ì‚° ì¤‘ ({model_handler['name']}, {len(texts)}ê°œ)...")

    # ëª¨ë¸ í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
    embeddings = model_handler['embed_texts'](texts)

    # Qdrantì— ì €ì¥í•  'Point' ê°ì²´ ìƒì„±
    points = [
        PointStruct(id=ch["id"], vector=vec, payload=ch)
        for vec, ch in zip(embeddings, chunks)
    ]

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—…ë¡œë“œ
    total_points = len(points)
    print(f"    [QDRANT] ë°°ì¹˜ ì—…ë¡œë“œ ì‹œì‘ (ì´ {total_points}ê°œ, ë°°ì¹˜ í¬ê¸°: {batch_size})...")

    for i in range(0, total_points, batch_size):
        batch = points[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total_points + batch_size - 1) // batch_size

        print(f"      - ë°°ì¹˜ {batch_num}/{total_batches} ì—…ë¡œë“œ ì¤‘ ({len(batch)}ê°œ)...")
        # 'upsert'ëŠ” IDê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€í•˜ê³ , IDê°€ ì´ë¯¸ ìˆìœ¼ë©´ ë®ì–´ì“°ëŠ” 'ì•ˆì „í•œ' ëª…ë ¹ì–´ì…ë‹ˆë‹¤.
        client.upsert(collection_name=collection_name, points=batch, wait=True)

    print(f"    [QDRANT] {total_points}ê°œ ë²¡í„° ì—…ë¡œë“œ/ì—…ë°ì´íŠ¸ ì™„ë£Œ.")



# ----------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ê¸°
# ----------------------------------------------------
def main_upload():
    print("--- (1/3) ì—…ë¡œë“œ ì‹œì‘: ì„¤ì • ë° í‚¤ ë¡œë“œ ---")

    try:
        # .env íŒŒì¼ì—ì„œ í•„ìš”í•œ í‚¤ë¥¼ ëª¨ë‘ ë¡œë“œí•©ë‹ˆë‹¤.
        load_dotenv()  # .env íŒŒì¼ ë¡œë“œ
        keys = {
            'qdrant_url': os.getenv(CONFIG_UPLOAD.QDRANT_URL_KEY),
            'qdrant_api': os.getenv(CONFIG_UPLOAD.QDRANT_API_KEY),
            'openai': os.getenv(CONFIG_UPLOAD.OPENAI_API_KEY)
        }

        # í•„ìˆ˜ í‚¤ í™•ì¸
        missing_keys = [k for k, v in keys.items() if not v]
        if missing_keys:
            raise ValueError(f"ëˆ„ë½ëœ í™˜ê²½ ë³€ìˆ˜: {missing_keys}")

        # Qdrant DBì— ì—°ê²°í•©ë‹ˆë‹¤. (timeout ì¦ê°€: ëŒ€ìš©ëŸ‰ ì—…ë¡œë“œ ëŒ€ë¹„)
        qdrant_client = QdrantClient(
            url=keys['qdrant_url'],
            api_key=keys['qdrant_api'],
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ (ê¸°ë³¸ê°’: 60ì´ˆ)
        )
        print("  [ë©”ì¸] Qdrant ë° API í‚¤ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸš¨ [ì˜¤ë¥˜] í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨. .env íŒŒì¼ì— í•„ìš”í•œ í‚¤ 3ê°œë¥¼ ëª¨ë‘ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print(f"    í•„ìš”í•œ í‚¤: {CONFIG_UPLOAD.QDRANT_URL_KEY}, {CONFIG_UPLOAD.QDRANT_API_KEY}, {CONFIG_UPLOAD.OPENAI_API_KEY}")
        print(f"    ì˜¤ë¥˜ ìƒì„¸: {e}")
        return

    print("\n--- (2/3) ë°ì´í„° ì¤€ë¹„: ë¡œë“œ, ì •ì œ, ì²­í‚¹ ---")

    # 1. ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ê¸°ë°˜ ì²­í¬(cisg_chunks.json)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    raw_text = load_document(CONFIG_UPLOAD.DOCUMENT_PATH)
    base_chunks_raw = load_base_chunks(CONFIG_UPLOAD.BASE_CHUNKS_PATH)
    
    # 2. ì²­í¬ì˜ 'start'/'end' ìœ„ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (ë§¤ìš° ì¤‘ìš”)
    base_chunks_ready = attach_chunk_spans(raw_text, base_chunks_raw)
    
    if not base_chunks_ready:
        print("ğŸš¨ [ì˜¤ë¥˜] ìœ íš¨í•œ ê¸°ë°˜ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤. BASE_CHUNKS_PATH íŒŒì¼ê³¼ DOCUMENT_PATH íŒŒì¼ì˜ ë‚´ìš©ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
        
    # 3. (2)ë²ˆ ì„¤ì •ì—ì„œ ì„ íƒí•œ 'ì²­í‚¹ ì „ëµ'ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    # ì˜ˆ: 'Paragraph'ë¥¼ ì„ íƒí–ˆë‹¤ë©´, 'Ho_Segmented' ì²­í¬ë“¤ì„ 'Paragraph' ë‹¨ìœ„ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.
    chunks_to_upload = merge_chunks(base_chunks_ready, CONFIG_UPLOAD.CHUNK_STRATEGY, raw_text)

    if not chunks_to_upload:
        print(f"ğŸš¨ [ì˜¤ë¥˜] '{CONFIG_UPLOAD.CHUNK_STRATEGY}' ì „ëµìœ¼ë¡œ ì²­í¬ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print("\n--- (3/3) ëª¨ë¸ ë¡œë“œ ë° ì—…ë¡œë“œ ì‹¤í–‰ ---")

    try:
        # 4. (2)ë²ˆ ì„¤ì •ì—ì„œ ì„ íƒí•œ 'ì„ë² ë”© ëª¨ë¸'ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        model_handler = get_model_handler(CONFIG_UPLOAD.MODEL_NAME, keys)
        
        # 5. Qdrant ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        # (ì£¼ì˜: ê¸°ì¡´ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ì™€ ë‹¬ë¦¬ 'recreate'(ì‚­ì œ)ë¥¼ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)
        create_collection_if_not_exists(
            qdrant_client, 
            CONFIG_UPLOAD.COLLECTION_NAME, 
            model_handler['dim']
        )
        
        # 6. ìµœì¢… ì²­í¬ë¥¼ ì„ë² ë”©í•˜ì—¬ Qdrantì— ì—…ë¡œë“œ(Upsert)í•©ë‹ˆë‹¤.
        upload_to_qdrant(
            qdrant_client,
            CONFIG_UPLOAD.COLLECTION_NAME,
            model_handler,
            chunks_to_upload
        )
        
        print(f"\nğŸ‰ === ì—…ë¡œë“œ ì„±ê³µ! ===")
        print(f"  - ì»¬ë ‰ì…˜: {CONFIG_UPLOAD.COLLECTION_NAME}")
        print(f"  - ì²­í¬ ìˆ˜: {len(chunks_to_upload)}ê°œ")
        print(f"  - ëª¨ë¸: {CONFIG_UPLOAD.MODEL_NAME}")

    except Exception as e:
        print(f"ğŸš¨ [ì˜¤ë¥˜] ì—…ë¡œë“œ ì‘ì—… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc() # ìƒì„¸ ì˜¤ë¥˜ ë‚´ì—­ ì¶œë ¥




# --- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ CISG ë¬¸ì„œ Qdrant ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    print("="*60)
    print(f"\nğŸ“‹ ì„¤ì • ì •ë³´:")
    print(f"  - ì»¬ë ‰ì…˜: {CONFIG_UPLOAD.COLLECTION_NAME}")
    print(f"  - ì„ë² ë”© ëª¨ë¸: {CONFIG_UPLOAD.MODEL_NAME}")
    print(f"  - ì²­í‚¹ ì „ëµ: {CONFIG_UPLOAD.CHUNK_STRATEGY}")
    print(f"  - ë¬¸ì„œ ê²½ë¡œ: {CONFIG_UPLOAD.DOCUMENT_PATH}")
    print(f"  - ì²­í¬ ê²½ë¡œ: {CONFIG_UPLOAD.BASE_CHUNKS_PATH}")
    print()

    main_upload()